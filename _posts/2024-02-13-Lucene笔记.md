Some Lucene notes,ES like a blackbox when we using it,and threre're some confusing features ,why docvalue can help sort,why filter not affect the score of documents,etc. But when i dive into Lucece documents, all answer is there. Let's looking under the hood.

Basic Lucene Definitions
The fundamental concepts of Lucene are index,document,field and term.
> An Index contains a sequence of documents.
> A document contains a sequence of fields.
> A field is a named sequence of terms.
> A term is a string ( the same string in different fileds is considered as a different term,thus a term is a pair of strings,the first naming the field, the second naming text within the field.)

Lucene can analogous to a database,Lucene also provides the ability of storing and searching data,what's the most important part of a database,absolutely it's data structure,like the b tree in mysql,the heap table in postgres.Lucene implements in a different way.

There're some keywords for performance,data compression,sequence store(for binary search),let's deep into now.

There's a trade off between compression ratio and compression/decompression rate,Lucene use LZ4 as its compression algorithm,with default 8KB block and BEST_SPEED compression method,which focus more on speed than on compression ratio.With IndexWriterConfig,you can change this default behavior,if you need higher compression(such as log files,HTML or plain text),you can choose BEST_COMPRESSION,and increase block size(smaller block size will more friendly for decompression).

Lucene stored files are represented by three files: a files data file,a fileds index file and a field meta file,there usage is shown below:
|file type|file extension(on disk)|content|
|-----|-----|-----|
|data file|.fdt|store all documents,with 8kB compression block(default value),documents are append to a byte[] in memory(which we called trunk),and will be compressed and flushed to disk when reach 80kB (every trunk contains a metadata header and compressed data after they're compressed,the headers record the length of origin trunk,the decompressor can stop when enough informations have been decompressed.The metadata header is lightweight,it's overhead less than 0.5% even if origin data is incompressible).|
|index file|.fdx|this file has two monotonic arrays,one for store the Doc Ids for the documents in datafile,another stores the offsets in data file of each documents,so when search documents by id,we can using binary search in the ids array and retrieve associated offset in the another array.|
|meta file|.fdm|the metadata about the monotonic arrays in the index file.|
